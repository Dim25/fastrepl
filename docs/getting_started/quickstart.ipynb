{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install fastrepl\n",
    "```\n",
    "\n",
    "You can find all releases [here](https://pypi.org/project/fastrepl).\n",
    "\n",
    "## Setup FastREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using it in a script\n",
    "import fastrepl\n",
    "\n",
    "# When using it in a notebook\n",
    "import fastrepl.repl as fastrepl\n",
    "\n",
    "fastrepl.LLMCache.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We will use [daily_dialog](https://huggingface.co/datasets/daily_dialog) from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"daily_dialog\", split=\"test\")\n",
    "ds = ds.shuffle(12)\n",
    "ds = ds.select(range(300))\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub(r\"\\s+([,.'!?])\", r\"\\1\", text.strip())\n",
    "\n",
    "\n",
    "def get_input(row):\n",
    "    msgs = [clean(msg) for msg in row[\"dialog\"]]\n",
    "    row[\"input\"] = \"\\n\".join(msgs)\n",
    "    return row\n",
    "\n",
    "\n",
    "ds = ds.map(get_input, remove_columns=[\"dialog\", \"act\", \"emotion\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluator\n",
    "\n",
    "Here, we are doing simple classifiction, but there are two interesting points.\n",
    "\n",
    "1. You can pass nearly any model for evaluation. (Thanks to [LiteLLM](https://github.com/BerriAI/litellm)).\n",
    "2. **`fastrepl` enhances accuracy by reducing [bias](/guides/dealing_with_bias.md)**. `position_debias_strategy` is one example, which ensures that the order of labels doesn't affect the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = fastrepl.Evaluator(\n",
    "    pipeline=[\n",
    "        fastrepl.LLMClassificationHead(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            context=\"You will receive casual conversation between two people.\",\n",
    "            labels={\n",
    "                \"FUN\": \"at least one of the two people try to be funny and entertain.\",\n",
    "                \"NOT_FUN\": \"given conversation lacks humor or entertainment value.\",\n",
    "            },\n",
    "            position_debias_strategy=\"consensus\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluator\n",
    "\n",
    "Here are some notes about running the evaluator:\n",
    "\n",
    "1. `ThreadPool` is used to make it faster (controlled by the `NUM_THREADS` environment variable).\n",
    "2. Any errors from different LLM providers are properly handled and retried with backoff if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c114a243add24664a72f5abde894ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 12:10:43,597 - 11720060928 - connectionpool.py-connectionpool:871 - WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x29cc827d0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")': /v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 12:11:09,697 - 11377176576 - _common.py-_common:105 - INFO: Backing off completion(...) for 2.5s (fastrepl.llm.RetryConstantException)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'prediction'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fastrepl.LocalRunner(evaluator=evaluator, dataset=ds).run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting point to note is that, due to the due to `position_debias_strategy=\"consensus\"`, if the order of the labels affects the result, `fastrepl` will return `None`. It will return more meaningful value in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"prediction\"].count(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4449760765550239"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = result[\"prediction\"].count(\"FUN\")\n",
    "nf = result[\"prediction\"].count(\"NOT_FUN\")\n",
    "\n",
    "f / (f + nf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
