# Caching

- Currently we use in-memory caching using [model and prompt as a key](https://docs.litellm.ai/docs/caching). 

- Eventually we will have our own implementation of disk-based caching.
