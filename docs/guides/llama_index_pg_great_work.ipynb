{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Evaluation on LlamaIndex built-in evaluation\n",
    "\n",
    "**This is WIP**\n",
    "\n",
    "`LlamaIndex` has good documentaion and [built-in support](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/usage_pattern.html) for evaluation. However, in the documentation, it only uses `GPT-4`.\n",
    "\n",
    "Is it possible to achieve similar accuracy using other models? Let's explore this with `fastrepl`!\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq llama_index==\"0.8.22\" pydantic nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleWebPageReader,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    ")\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "\n",
    "[How to do great work](http://paulgraham.com/greatwork.html) is wonderful blog post by Paul Graham. With `SimpleWebPageReader`, we can easily load the documents and get the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"http://paulgraham.com/greatwork.html\"]\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(urls)\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, service_context=service_context\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n",
    "\n",
    "To run a `Meta-Evaluation`, we need some kind of dataset. (Process of meta-eval is explained in detailed [here](https://docs.fastrepl.com/getting_started/quickstart#meta-evaluation)). The good thing is that `LllamaIndex` has `DatasetGenerator`!\n",
    "\n",
    "We first run a query and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow your heart.\n"
     ]
    }
   ],
   "source": [
    "query = \"To do great work, should I follow my heart or my head?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, for sure. Let's look into the sources too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 802410e3-50d2-496a-8e57-de6a167647af): This is how practically everyone who's done great work\n",
      "has done it, from painters to physicists.Steps two and four will require hard work.It may not be possible to prove\n",
      "that you have to work hard to do great things, but the empirical evidence is\n",
      "on the scale of the evidence for mortality.That's why it's essential to work\n",
      "on something you're deeply interested in.Interest will drive you to work\n",
      "harder than mere diligence ever could.The three most powerful motives are curiosity, delight, and th...\n",
      "\n",
      "> Source (Doc id: 80b37615-357d-4f18-a412-88589ff37026): Since it matters so much for this cycle to be\n",
      "running in the right direction, it can be a good idea to switch to easier work\n",
      "when you're stuck, just so you start to get something done.One of the biggest mistakes ambitious people make is to allow setbacks to\n",
      "destroy their morale all at once, like a balloon bursting.You can inoculate\n",
      "yourself against this by explicitly considering setbacks a part of your\n",
      "process.Solving hard problems always involves some backtracking.Doing great work is a depth...\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources(length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 21:42:25,566 - 8036850496 - service_context.py-service_context:132 - WARNING: chunk_size_limit is deprecated, please specify chunk_size instead\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "questions = data_generator.generate_questions_from_nodes(num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37c8276595f49f5989b0ec3c86625b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from datasets import Dataset\n",
    "from llama_index import Response\n",
    "import fastrepl.repl as fastrepl\n",
    "\n",
    "\n",
    "def get_context(response: Response) -> List[str]:\n",
    "    return [context_info.node.get_content() for context_info in response.source_nodes]\n",
    "\n",
    "\n",
    "def get_input(query: str, r: Response) -> str:\n",
    "    response = r.response\n",
    "    context = get_context(r)\n",
    "    return f\"Query: {query}, Response: {response}, Context: {context}\"\n",
    "\n",
    "\n",
    "_ds = Dataset.from_dict({\"question\": questions})\n",
    "\n",
    "\n",
    "def transform(row):\n",
    "    query = row[\"question\"]\n",
    "    response = query_engine.query(query)\n",
    "    row[\"input\"] = get_input(query, response)\n",
    "    return row\n",
    "\n",
    "\n",
    "ds = _ds.map(transform, remove_columns=[\"question\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6799bcc3a2db421aaf681033ffdb06a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Query: What are the three qualities that the w...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Query: How does the author suggest figuring ou...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Query: What are the four steps the author outl...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Query: Why does the author emphasize the impor...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Query: How does the author suggest making your...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input prediction\n",
       "0  Query: What are the three qualities that the w...         NO\n",
       "1  Query: How does the author suggest figuring ou...        YES\n",
       "2  Query: What are the four steps the author outl...         NO\n",
       "3  Query: Why does the author emphasize the impor...         NO\n",
       "4  Query: How does the author suggest making your...        YES"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = fastrepl.Evaluator(\n",
    "    pipeline=[\n",
    "        fastrepl.LLMClassificationHead(\n",
    "            model=\"gpt-4\",\n",
    "            context=\"You will receive text containing query, response, and context information. You should evaluate the response based on the query and context.\",\n",
    "            labels={\n",
    "                \"YES\": \"response for the query is in line with the context.\",\n",
    "                \"NO\": \"response for the query is NOT in line with the context.\",\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = fastrepl.LocalRunner(evaluator, ds).run()\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/jerryjliu/llama_index/blob/9acd9297860824ebc2c9c47358c05f387c62cff5/llama_index/evaluation/base.py#L226\n",
    "\n",
    "[QueryResponseEvaluator](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/usage_pattern.html#evaluting-query-response-for-answer-quality) checks if the synthesized response matches the query + any source context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the three qualities that the work you...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the author suggest figuring out what ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the four steps the author outlines fo...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does the author emphasize the importance o...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the author suggest making yourself a ...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input prediction\n",
       "0  What are the three qualities that the work you...        YES\n",
       "1  How does the author suggest figuring out what ...        YES\n",
       "2  What are the four steps the author outlines fo...        YES\n",
       "3  Why does the author emphasize the importance o...        YES\n",
       "4  How does the author suggest making yourself a ...        YES"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "\n",
    "evaluator = QueryResponseEvaluator(service_context=service_context)\n",
    "\n",
    "results = []\n",
    "\n",
    "for query in _ds[\"question\"]:\n",
    "    response = query_engine.query(query)\n",
    "    result = evaluator.evaluate(query, response)\n",
    "    results.append(result)\n",
    "\n",
    "result2 = Dataset.from_dict({\"input\": _ds[\"question\"], \"prediction\": results})\n",
    "result2.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
